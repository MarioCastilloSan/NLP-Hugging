from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

#tokenizer = AutoTokenizer.from_pretrained("b2bFiles")

#model = AutoModelForSeq2SeqLM.from_pretrained("b2bFiles")


#To download this model
tokenizer = AutoTokenizer.from_pretrained("mrm8488/bert2bert-spanish-question-generation")
model = AutoModelWithLMHead.from_pretrained("mrm8488/bert2bert-spanish-question-generation")

#To save it once you download it
#tokenizer.save_pretrained("b2bFiles")
#model.save_pretrained("b2bFiles")

def get_question(answer, context, max_length=64):
  input_text = "answer: %s  context: %s </s>" % (answer, context)
  features = tokenizer([input_text], return_tensors='pt')

  output = model.generate(input_ids=features['input_ids'], 
               attention_mask=features['attention_mask'],
               max_length=max_length)

  return tokenizer.decode(output[0])

context = "Manuel creo un transformer con ayuda de HF basado en BERT"
answer = "Manuel"

print(get_question(answer, context))